---
title: "Modeling"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  dev.args = list(bg = "transparent"),
  comment = NA
  )
```

```{r}
library(tidyverse)
library(knitr)
library(lubridate)
library(ggcorrplot)
library(plotly)
library(scales)
library(randomForest)
library(caTools)

steam_theme = theme_classic() + 
  theme(
    panel.background = element_rect(fill = "transparent", color = NA), 
    plot.background = element_rect(fill = "transparent", color = NA),
    legend.background = element_rect(fill = "transparent", color = NA),
    legend.box.background = element_rect(fill = "transparent", color = NA),
    text = element_text(color = "#66c0f4")
    )

theme_set(steam_theme)
steam_df = read_csv("data/clean_steam_games.csv")
```


# Modeling Strategy

This additional analysis aims to predict the maximum concurrent player count (`peak_ccu`) for Steam games based on current metadata. By modeling the relationship between game attributes—such as price, genre, and release timing—and player traffic, we can identify the key drivers of a game's commercial performance.

We selected a Random Forest as our modeling algorithm. This decision was made by the some characteristics of the Steam dataset observed during [Exploratory Data Analysis](eda.html):

- **Weak Linearity**: Our correlation matrix revealed that relationships between variables are barely linear. Random Forest captures complex, non-linear interactions without requiring manual equation fitting.

- **Robustness to Outliers**: Steam data has a skewed distribution, where a few massive hits coexist with thousands of small games. Random Forest averages the results of multiple decision trees, preventing these extreme outliers from messing up the entire model.

Before fitting the Random Forest, we added a baseline:
Baseline Model : Linear Regression
This model assumes a linear relationship between features and log(Peak CCU).
It serves as a benchmark to quantify Random Forest’s non-linear improvement.

# Data Preparation

To address the extreme skewness of the data, we perform a log1p transformation ($y = \ln(x + 1)$) to `peak_ccu` and continuous features. This transformation, as we previously mentioned, allows the model to learn magnitudes.

```{r}
model_data = steam_df |>
  mutate(
    log_peak_ccu = log1p(peak_ccu),
    log_price = log1p(price),
    release_date = as.numeric(release_date),
    total_reviews = positive + negative,
    review_ratio = ifelse(total_reviews > 0, positive / total_reviews, 0),
    log_estimated_owners = log1p(estimated_owners),
    log_average_playtime = log1p(average_playtime)
  ) |>
  select(log_peak_ccu, log_price, log_estimated_owners, log_average_playtime, dlc_count, primary_genre, release_date, review_ratio) |>
  na.omit()
```

# Training Regressoor

We first set a seed to make sure our work is reproducible. The dataset was then split into training (80% of original data) and testing (20% of original data) sets. We then generated a basic model with only individual variables as a control group. Finally, we start training our Random Forest regressor with 500 decision trees and save this model as `rf_model`. The details of basic model and `rf_model` is given below.
```{r}
set.seed(8105)

split = sample.split(model_data$log_peak_ccu, SplitRatio = 0.8)
train_set = subset(model_data, split == TRUE)
test_set = subset(model_data, split == FALSE)

baseline_model <- lm(log_peak_ccu ~ ., data = train_set)

rf_model = randomForest(log_peak_ccu ~ ., 
                         data = train_set, 
                         ntree = 500, 
                         importance = TRUE)

print(rf_model)
```

# Evaluation


```{r}
valid_genres <- unique(train_set$primary_genre)
test_setnew <- test_set |>
  dplyr::filter(primary_genre %in% valid_genres)
log_pred_lm <- predict(baseline_model, newdata = test_setnew)
log_pred_rf <- predict(rf_model,     newdata = test_setnew)
real_actuals <- expm1(test_setnew$log_peak_ccu)
pred_lm      <- expm1(log_pred_lm)
pred_rf      <- expm1(log_pred_rf)

rmse_fun <- function(pred, obs) {
  sqrt(mean((pred - obs)^2))
}

lm_rmse <- rmse_fun(pred_lm, real_actuals)
rf_rmse <- rmse_fun(pred_rf, real_actuals)

lm_nrmse <- lm_rmse / mean(real_actuals)
rf_nrmse <- rf_rmse / mean(real_actuals)

results_table <- tibble::tibble(
  Model = c("Linear Regression", "Random Forest"),
  RMSE  = c(lm_rmse, rf_rmse),
  NRMSE = c(lm_nrmse, rf_nrmse)
)

knitr::kable(results_table, digits = 4)
```

We first generate the log-scale predictions. Then we reverse our log transformation with exponential equation to get the RMSE (Root Mean Squared Error) in actual player units. Additionally, we apply normalization to our RMSE to get NRMSE, where:
\[
NRMSE = \frac{RMSE}{Mean}
\]

NRMSE tells us the error percentage relative to the average game. The baseline model gives a RMSE of 142.43 and NRMSE of 2.99, and final model achieved a RMSE of 122.21	 and a Normalized RMSE of 2.56. Considering the large variance of the dataset, our model holds a fairly stable predictive capability.

# Model Diagnostics: Predicted vs Actual Peak CCU

To better compare the baseline linear model and the Random Forest, we visualize predicted versus actual peak CCU on a log–log scale for both models.


```{r}
diag_df <- tibble::tibble(
  actual   = real_actuals,
  pred_lm  = pred_lm,
  pred_rf  = pred_rf
)

set.seed(8105)
diag_sample <- diag_df |>
  dplyr::sample_n(size = min(3000, nrow(diag_df)))

p_lm <- ggplot(diag_sample, aes(x = actual, y = pred_lm)) +
  geom_point(alpha = 0.3, size = 0.7, color = "#66c0f4") +
  geom_abline(
    slope = 1, intercept = 0,
    linetype = "dashed",
    color = "#e6e9f0",
    linewidth = 0.5
  ) +
  scale_x_log10(labels = scales::comma) +
  scale_y_log10(labels = scales::comma) +
  labs(
    title = "Predicted vs Actual Peak CCU (Linear Regression)",
    x = "Actual Peak CCU",
    y = "Predicted Peak CCU"
  )

p_rf <- ggplot(diag_sample, aes(x = actual, y = pred_rf)) +
  geom_point(alpha = 0.3, size = 0.7, color = "#66c0f4") +
  geom_abline(
    slope = 1, intercept = 0,
    linetype = "dashed",
    color = "#e6e9f0",
    linewidth = 0.5
  ) +
  scale_x_log10(labels = scales::comma) +
  scale_y_log10(labels = scales::comma) +
  labs(
    title = "Predicted vs Actual Peak CCU (Random Forest)",
    x = "Actual Peak CCU",
    y = "Predicted Peak CCU"
  )

subplot(
  ggplotly(p_lm),
  ggplotly(p_rf),
  nrows = 1,
  margin = 0.05,
  titleX = TRUE,
  titleY = TRUE
)

```

Beyond these summary metrics, we also compared predicted versus actual peak CCU on the test set for both models. On the log–log scale, the Random Forest predictions cluster more tightly around the 45-degree reference line than those from the linear regression, especially for games at the extremes of the popularity distribution. The linear model tends to underestimate the largest hits and overestimate some mid-range titles, whereas the Random Forest tracks these non-linear patterns more closely. 

This diagnostic comparison is consistent with the RMSE and NRMSE results and provides visual evidence that the Random Forest is better suited to capturing complex relationships in the Steam dataset.