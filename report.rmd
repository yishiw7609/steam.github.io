---
title: "Written Report"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  dpi = 300
  )
```

```{r}
library(tidyverse)
library(plotly)
library(knitr)
library(lubridate)
library(ggcorrplot)
```

# Motivation

Video game industry is a highly competitive market where thousands of new products are released annually. For developers, understanding the specific factors that drive player engagement is important when allocating their resources. For players facing an overwhelming number of choices, identifying games that align with their interests is a challenge. This project is motivated by both the need to bridge this gap and our enthusiasm towards gaming: by analyzing real-world data from [Steam](https://en.wikipedia.org/wiki/Steam_(service)), we aim to build a predictive model that can estimate a game's popularity based on pre-release information.

# Related Work

Our work is inspired by the recent kick-off of [TGA](https://thegameawards.com/nominees/game-of-the-year): An once-a-year celebration of game developers and players, simply consider it as the Oscar of gaming industry.

# Initial Questions

Our major questions is: **What are the primary drivers of game popularity on Steam?**

As we explored the dataset, this question evolved into more specific hypotheses:
"Does a higher price point correlate with higher quality and thus more playtime?" "Is the relationship between price and engagement non-linear, perhaps differing for Indie vs. AAA titles?"
"Do critical reviews (Metacritic) serve as a better predictor of peak concurrent users than user reviews?"
"Can the number of DLCs act as a proxy for a game's longevity?"

# Data

## Data Source

The dataset used for this project is made possible by [FronkonGames](https://fronkongames.github.io/) and is sourced at [huggingface.co](https://huggingface.co/datasets/FronkonGames/steam-games-dataset).

## Scraping

The author provides an official python script to scrape this dataset, it can be viewed [here]("data/scrapping.py"). Additionally, we did some sampling 
from local. The chunk below converted a dataframe of 110000 rows to 10629 rows to pass Github's file uploading size. It is not meant to be ran.
```{r}
# df = read_csv("steam_games.csv") |>
#   filter(!is.na(average_playtime),
#          !is.na(peak_ccu),
#          !is.na(median_playtime))

# write_csv(df, "steam_games.csv")
```

## Cleaning

During the cleaning process, our pipeline aims to:

-Make sure variable names are consistent across columns

-Select only the unique identifier and necessary variables for our popularity analysis

-Make sure continuous variables are numeric

-Make sure `release_date` is in date format

-Convert the `estimated_owners` from a range to a numeric value by taking the midpoint

-Make sure no zeros or NAs are present in `average_playtime`, `median_playtime` and `peak_ccu` as they are meaningless in these columns

```{r}
steam_df = read_csv("data/steam_games.csv") |>
  janitor::clean_names() |>
  select(
    app_id, name, release_date, price, required_age,
    estimated_owners, peak_ccu, average_playtime, median_playtime,
    achievements, dlc_count, developers, publishers,
    categories, genres, tags, positive, negative
  ) |>
  mutate(
    price = as.numeric(price),
    required_age = as.numeric(required_age),
    achievements = as.numeric(achievements),
    dlc_count = as.numeric(dlc_count),
    positive = as.numeric(positive),
    negative = as.numeric(negative),
    release_date = mdy(release_date)
  ) |>
  mutate(estimated_owners = str_extract(estimated_owners, "\\d+"),
         estimated_owners = as.numeric(estimated_owners)) |>
  filter(
    average_playtime > 0,
    median_playtime > 0,
    peak_ccu > 0
  )

```

## Key Variables

**app_id**: Unique identifier of each game

**name**: Game name on Steam

**release_date**: Official releasing time in yyyy-mm-dd

**price**: Price in USD

**estimated_owners**: Rough estimate of owners

**peak_ccu**: Peak number of online concurrent users

**average_playtime, median_playtime**: Playtime since March 2009, in minutes

**dlc_count**: Number of DLCs (expansion or extra chapters sold separately from the game)

**developers, publishers**: Creator of the game / Seller of the game

**categories**: Type of the game

**genres**: Theme of the game

**tags**: Tags added by users

**positive, negative**: Number of positive / negative reviews by user on Steam

# Exploratory Data Analysis

In this study, we define "popularity" with playtime and peak concurrent users. Let's first look at the distribution of average time of these games. Note that playtime data is heavyly tailed (most games are barely played, and a few games dominate), so we apply a log transformation here to see a meaningful distribution and avoid outliers.
```{r}
average_playtime_distribution = steam_df |> 
  ggplot(aes(x = average_playtime)) +
  geom_histogram(bins = 50) +
  scale_x_continuous(trans = "log10") +
  labs(
    title = "Distribution of Average Playtime (Log Scale)",
    x = "Average playtime (minutes, log10 scale)"
  )

average_playtime_distribution
```


```{r}
steam_numeric = steam_df |>
  select(price, required_age, estimated_owners, 
         peak_ccu, average_playtime, median_playtime,
         achievements, dlc_count, positive, negative)

steam_numeric |>
  cor(use = "complete.obs") |>
  ggcorrplot(lab = TRUE)
```

# Additional Analysis


# Conclusion and Findings
